================================================================================
COMPARACIÓN EXPERIMENTAL: A* vs Q-LEARNING
================================================================================

RESUMEN DEL EXPERIMENTO
----------------------------------------
Total de experimentos: 432
Ejecuciones A*: 216
Ejecuciones Q-Learning: 216

TASAS DE ÉXITO
----------------------------------------
A*: 215/216 (99.5%)
Q-Learning: 215/216 (99.5%)

RENDIMIENTO POR DENSIDAD DE OBSTÁCULOS
----------------------------------------

Densidad 0.0:
  Tasa de Éxito - A*: 100.0%, Q-Learning: 100.0%
  Longitud Promedio - A*: 6.6, Q-Learning: 6.7
  Diferencia de Pasos: +0.1 pasos (Q-Learning vs A*)
  Tiempo Promedio - A*: 0.000s, Q-Learning: 0.126s

Densidad 0.1:
  Tasa de Éxito - A*: 100.0%, Q-Learning: 100.0%
  Longitud Promedio - A*: 6.8, Q-Learning: 6.9
  Diferencia de Pasos: +0.1 pasos (Q-Learning vs A*)
  Tiempo Promedio - A*: 0.000s, Q-Learning: 0.123s

Densidad 0.2:
  Tasa de Éxito - A*: 100.0%, Q-Learning: 100.0%
  Longitud Promedio - A*: 7.1, Q-Learning: 7.2
  Diferencia de Pasos: +0.1 pasos (Q-Learning vs A*)
  Tiempo Promedio - A*: 0.000s, Q-Learning: 0.122s

Densidad 0.3:
  Tasa de Éxito - A*: 98.1%, Q-Learning: 98.1%
  Longitud Promedio - A*: 7.4, Q-Learning: 7.5
  Diferencia de Pasos: +0.1 pasos (Q-Learning vs A*)
  Tiempo Promedio - A*: 0.000s, Q-Learning: 0.140s

RENDIMIENTO POR DISTANCIA INICIO-META
----------------------------------------

Distancia Cercana:
  Tasa de Éxito - A*: 100.0%, Q-Learning: 100.0%
  Longitud Promedio - A*: 3.1, Q-Learning: 3.1
  Diferencia de Pasos: +0.0 pasos (Q-Learning vs A*)

Distancia Media:
  Tasa de Éxito - A*: 100.0%, Q-Learning: 100.0%
  Longitud Promedio - A*: 7.2, Q-Learning: 7.3
  Diferencia de Pasos: +0.1 pasos (Q-Learning vs A*)

Distancia Lejana:
  Tasa de Éxito - A*: 98.6%, Q-Learning: 98.6%
  Longitud Promedio - A*: 10.6, Q-Learning: 10.8
  Diferencia de Pasos: +0.2 pasos (Q-Learning vs A*)

RENDIMIENTO POR TASA DE APRENDIZAJE (alpha)
----------------------------------------

alpha = 0.1:
  Tasa de Éxito Q-Learning: 100.0%
  Longitud Promedio Q-Learning: 7.1
  Tiempo Promedio Q-Learning: 0.123s

alpha = 0.3:
  Tasa de Éxito Q-Learning: 99.1%
  Longitud Promedio Q-Learning: 7.0
  Tiempo Promedio Q-Learning: 0.132s

HALLAZGOS PRINCIPALES
----------------------------------------
Mejor rendimiento Q-Learning:
  Configuración: d0.0_distclose_a0.1_g0.9_rbasic
  Longitud del camino: 3 pasos
  Tiempo: 0.103s

Peor rendimiento Q-Learning:
  Configuración: d0.3_distfar_a0.1_g0.9_rbasic
  Longitud del camino: 14 pasos
  Tiempo: 0.185s

RECOMENDACIONES
----------------------------------------
1. A* es óptimo para longitud de camino cuando el entorno es completamente conocido
2. Q-Learning requiere ajuste para diferentes complejidades del entorno
3. Mayor densidad de obstáculos degrada el rendimiento de Q-Learning más que A*
4. El tiempo de entrenamiento de Q-Learning aumenta significativamente con la complejidad
5. Considerar enfoques híbridos para aplicaciones en tiempo real